{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Core ML Task: Robust Loss Functions for Noisy Labels (CIFAR-10)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Standard Libraries ---\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- PyTorch Libraries ---\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "# --- Environment Setup ---\n",
        "try:\n",
        "    # Check if running in Google Colab for Drive mounting\n",
        "    from google.colab import drive\n",
        "    COLAB_ENV = True\n",
        "except ImportError:\n",
        "    COLAB_ENV = False\n",
        "\n",
        "# Print library versions and environment\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"Torchvision Version: {torchvision.__version__}\")\n",
        "print(f\"Running in Colab: {COLAB_ENV}\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Configuration\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Training Parameters ---\n",
        "EPOCHS = 50           # Number of epochs for each training run\n",
        "LEARNING_RATE = 0.1   # Initial learning rate for SGD\n",
        "BATCH_SIZE = 128      # Training batch size\n",
        "SEED = 42             # Random seed for reproducibility\n",
        "WEIGHT_DECAY = 5e-4   # Weight decay for SGD optimizer\n",
        "\n",
        "# --- Experiment Selection ---\n",
        "RUN_SYMMETRIC = True    # Run experiments with symmetric noise\n",
        "RUN_ASYMMETRIC = False  # Run experiments with asymmetric noise (Bonus)\n",
        "\n",
        "# Select specific loss functions to run by listing their keys from `all_losses`.\n",
        "# Set to None to run ALL defined losses.\n",
        "# Example: LOSSES_TO_RUN = ['CE', 'NCE (s=1)', 'APL (NCE+MAE)']\n",
        "LOSSES_TO_RUN = ['CE', 'NCE (s=1)', 'APL (NCE+MAE)', 'APL (NCE+RCE)']\n",
        "\n",
        "# Define the noise rates for each type of noise experiment\n",
        "SYMMETRIC_NOISE_RATES = [0.0, 0.2, 0.4, 0.6, 0.8]\n",
        "ASYMMETRIC_NOISE_RATES = [0.0, 0.1, 0.2, 0.3, 0.4]\n",
        "\n",
        "# --- Model Selection ---\n",
        "USE_SIMPLE_CNN = False # If True, use the basic CNN architecture\n",
        "USE_RESNET9 = True     # If True, use the ResNet9 architecture\n",
        "# If both are False, ResNet18 will be used by default\n",
        "\n",
        "# --- Results Saving ---\n",
        "SAVE_RESULTS = False # Set to True to enable saving results periodically\n",
        "# Define results directory (local default, overridden if on Colab with saving)\n",
        "RESULTS_DIR = 'CoreML_Results_ResNet9'\n",
        "if COLAB_ENV and SAVE_RESULTS:\n",
        "    RESULTS_DIR = '/content/drive/MyDrive/CoreML_Results_ResNet9'\n",
        "    try:\n",
        "        print(\"Attempting to mount Google Drive...\")\n",
        "        drive.mount('/content/drive')\n",
        "        os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "        print(f\"Google Drive mounted. Results directory: {RESULTS_DIR}\")\n",
        "    except Exception as e:\n",
        "        print(f\"WARN: Drive mount/creation failed: {e}. Disabling saving.\")\n",
        "        SAVE_RESULTS = False\n",
        "elif SAVE_RESULTS:\n",
        "     # Ensure local directory exists if saving locally\n",
        "     os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "     print(f\"Results directory (local): {RESULTS_DIR}\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. Data Preparation\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\nSetting up data transformations...\")\n",
        "# Training transforms include augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
        "    )\n",
        "])\n",
        "# Testing transforms include only normalization\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
        "    )\n",
        "])\n",
        "\n",
        "print(\"Loading CIFAR-10 dataset...\")\n",
        "try:\n",
        "    # Load raw training data (no transforms yet)\n",
        "    train_data_clean = datasets.CIFAR10(\n",
        "        root=\"data\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=None\n",
        "    )\n",
        "    # Load test data with test transforms\n",
        "    test_data = datasets.CIFAR10(\n",
        "        root=\"data\",\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform_test\n",
        "    )\n",
        "    class_names = train_data_clean.classes\n",
        "    num_classes = len(class_names)\n",
        "    print(\n",
        "        f\"CIFAR-10 loaded: {len(train_data_clean)} train, \"\n",
        "        f\"{len(test_data)} test images.\"\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"FATAL: Error loading CIFAR-10 dataset: {e}\")\n",
        "    exit(1) # Exit script if data loading fails\n",
        "\n",
        "\n",
        "# --- Noisy Dataset Class ---\n",
        "class NoisyCIFAR10(Dataset):\n",
        "    \"\"\"\n",
        "    A Dataset wrapper for CIFAR-10 that injects label noise.\n",
        "\n",
        "    Args:\n",
        "        dataset: The original CIFAR-10 dataset instance.\n",
        "        noise_type (str): Type of noise ('symmetric' or 'asymmetric').\n",
        "        eta (float): Noise rate (probability of flipping a label).\n",
        "        num_classes (int): Number of classes in the dataset.\n",
        "        transform (callable, optional): Transform to be applied on samples.\n",
        "        seed (int): Random seed for noise generation reproducibility.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, dataset, noise_type='symmetric', eta=0.0,\n",
        "        num_classes=10, transform=None, seed=42\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.noise_type = noise_type\n",
        "        self.eta = eta\n",
        "        self.num_classes = num_classes\n",
        "        self.transform = transform\n",
        "        self.seed = seed\n",
        "\n",
        "        try:\n",
        "            original_labels = np.array(dataset.targets)\n",
        "        except AttributeError:\n",
        "            original_labels = np.array([s[1] for s in dataset.samples])\n",
        "\n",
        "        self.original_labels = original_labels.copy()\n",
        "        self.noisy_labels = self._create_noisy_labels()\n",
        "\n",
        "    def _create_noisy_labels(self):\n",
        "        \"\"\"Creates and returns the array of noisy labels.\"\"\"\n",
        "        labels = self.original_labels.copy()\n",
        "        np.random.seed(self.seed + int(self.eta * 100))\n",
        "\n",
        "        if self.eta > 0:\n",
        "            if self.noise_type == 'symmetric':\n",
        "                self._add_symmetric_noise(labels)\n",
        "            elif self.noise_type == 'asymmetric':\n",
        "                self._add_asymmetric_noise(labels)\n",
        "            else:\n",
        "                 print(f\"Warning: Unknown noise_type '{self.noise_type}'.\")\n",
        "        return labels\n",
        "\n",
        "    def _add_symmetric_noise(self, labels):\n",
        "        \"\"\"Applies symmetric noise in-place to the labels array.\"\"\"\n",
        "        mask = np.random.rand(len(labels)) < self.eta\n",
        "        indices_to_corrupt = np.where(mask)[0]\n",
        "\n",
        "        num_flipped = 0\n",
        "        for i in indices_to_corrupt:\n",
        "            original_label = labels[i]\n",
        "            possible_labels = list(range(self.num_classes))\n",
        "            if original_label in possible_labels:\n",
        "                 possible_labels.remove(original_label)\n",
        "            if possible_labels:\n",
        "                labels[i] = np.random.choice(possible_labels)\n",
        "                num_flipped += 1\n",
        "        # print(f\"    Symmetric noise ({self.eta*100:.0f}%): {num_flipped} labels flipped.\")\n",
        "\n",
        "    def _add_asymmetric_noise(self, labels):\n",
        "        \"\"\"Applies asymmetric noise in-place to the labels array.\"\"\"\n",
        "        source_classes = [9, 2, 4, 3, 5] # Truck, Bird, Deer, Cat, Dog\n",
        "        noise_map = {9: 1, 2: 0, 4: 7, 3: 5, 5: 3}\n",
        "\n",
        "        # Determine candidates based on original labels\n",
        "        indices_in_source_classes = np.isin(\n",
        "            self.original_labels, source_classes\n",
        "        )\n",
        "        mask_random = np.random.rand(len(labels)) < self.eta\n",
        "        indices_to_corrupt = np.where(\n",
        "            indices_in_source_classes & mask_random\n",
        "        )[0]\n",
        "\n",
        "        num_flipped = 0\n",
        "        for i in indices_to_corrupt:\n",
        "            current_label = labels[i]\n",
        "            # Only flip if the *current* label is one of the source classes\n",
        "            if current_label in noise_map:\n",
        "                 labels[i] = noise_map[current_label]\n",
        "                 num_flipped += 1\n",
        "        # print(\n",
        "        #     f\"    Asymmetric noise ({self.eta*100:.0f}%): \"\n",
        "        #     f\"{num_flipped} labels flipped from sources.\"\n",
        "        # )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Retrieves an image and its (potentially noisy) label.\"\"\"\n",
        "        img, _ = self.dataset[index]\n",
        "        label = self.noisy_labels[index]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, torch.tensor(label).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. Loss Functions\n",
        "# ==============================================================================\n",
        "print(\"\\nDefining Loss Functions...\")\n",
        "\n",
        "class CrossEntropyLoss(nn.Module):\n",
        "    \"\"\"Standard Cross Entropy Loss wrapper.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        return self.cross_entropy(logits, labels)\n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"Focal Loss implementation.\"\"\"\n",
        "    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n",
        "        super().__init__()\n",
        "        if reduction not in ['mean', 'sum', 'none']:\n",
        "            raise ValueError(f\"Invalid reduction type: {reduction}\")\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        ce_loss = F.cross_entropy(logits, labels, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_term = self.alpha * (1 - pt)**self.gamma\n",
        "        loss = focal_term * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        else: # 'none'\n",
        "            return loss\n",
        "\n",
        "\n",
        "class NormalizedCrossEntropy(nn.Module):\n",
        "    \"\"\"Normalized Cross Entropy Loss (NCE).\"\"\"\n",
        "    def __init__(self, num_classes=10, scale=1.0):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        logits_scaled = logits / self.scale\n",
        "        log_softmax = F.log_softmax(logits_scaled, dim=1)\n",
        "        loss_numerator = -log_softmax.gather(\n",
        "            1, labels.unsqueeze(1)\n",
        "        ).squeeze(1)\n",
        "        loss_denominator = -log_softmax.sum(dim=1)\n",
        "        loss_denominator = torch.clamp(loss_denominator, min=1e-6)\n",
        "        loss = loss_numerator / loss_denominator\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "class NormalizedFocalLoss(nn.Module):\n",
        "    \"\"\"Normalized Focal Loss (NFL).\"\"\"\n",
        "    def __init__(\n",
        "        self, num_classes=10, alpha=1.0, gamma=2.0, scale=1.0,\n",
        "        reduction='mean'\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if reduction not in ['mean', 'sum', 'none']:\n",
        "            raise ValueError(f\"Invalid reduction type: {reduction}\")\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.scale = scale\n",
        "        self.num_classes = num_classes\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        logits_scaled = logits / self.scale\n",
        "        probs = F.softmax(logits_scaled, dim=1)\n",
        "        pt = probs.gather(1, labels.unsqueeze(1)).squeeze(1)\n",
        "        pt = torch.clamp(pt, min=1e-6, max=1.0 - 1e-6)\n",
        "\n",
        "        focal_term_true = (1 - pt)**self.gamma\n",
        "        log_pt = torch.log(pt)\n",
        "\n",
        "        focal_term_all = (1 - probs)**self.gamma\n",
        "        log_probs_all = F.log_softmax(logits_scaled, dim=1)\n",
        "        denominator = torch.sum(focal_term_all * (-log_probs_all), dim=1)\n",
        "        denominator = torch.clamp(denominator, min=1e-6)\n",
        "\n",
        "        numerator = focal_term_true * (-log_pt)\n",
        "        loss = self.alpha * numerator / denominator\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        else: # 'none'\n",
        "            return loss\n",
        "\n",
        "\n",
        "class MeanAbsoluteErrorLoss(nn.Module):\n",
        "    \"\"\"Mean Absolute Error Loss (Passive Loss).\"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        labels_one_hot = F.one_hot(\n",
        "            labels, num_classes=self.num_classes\n",
        "        ).float()\n",
        "        mae_sum_per_sample = torch.abs(probs - labels_one_hot).sum(dim=1)\n",
        "        return (mae_sum_per_sample / self.num_classes).mean()\n",
        "\n",
        "\n",
        "class ReverseCrossEntropy(nn.Module):\n",
        "    \"\"\"Reverse Cross Entropy Loss (Passive Loss).\"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        probs = torch.clamp(probs, min=1e-7, max=1.0) # Avoid log(0)\n",
        "        labels_one_hot = F.one_hot(\n",
        "            labels, num_classes=self.num_classes\n",
        "        ).float()\n",
        "        # Loss for incorrect classes: -log(1 - p_k)\n",
        "        loss_incorrect = -torch.log(1.0 - probs + 1e-7)\n",
        "        # Zero out contribution from the correct class\n",
        "        loss_incorrect = loss_incorrect * (1.0 - labels_one_hot)\n",
        "        return loss_incorrect.sum(dim=1).mean()\n",
        "\n",
        "\n",
        "class APL(nn.Module):\n",
        "    \"\"\"Active-Passive Loss framework.\"\"\"\n",
        "    def __init__(\n",
        "        self, active_loss: nn.Module, passive_loss: nn.Module,\n",
        "        alpha: float = 1.0, beta: float = 1.0\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.active_loss = active_loss\n",
        "        self.passive_loss = passive_loss\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        active_l = self.active_loss(logits, labels)\n",
        "        passive_l = self.passive_loss(logits, labels)\n",
        "\n",
        "        if torch.isnan(active_l) or torch.isinf(active_l) or \\\n",
        "           torch.isnan(passive_l) or torch.isinf(passive_l):\n",
        "            print(\n",
        "                f\"WARN: NaN/Inf in APL! A:{active_l.item():.4f}, \"\n",
        "                f\"P:{passive_l.item():.4f}.\"\n",
        "            )\n",
        "            return torch.tensor(\n",
        "                1000.0, requires_grad=True\n",
        "            ).to(logits.device)\n",
        "\n",
        "        return self.alpha * active_l + self.beta * passive_l\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. Model Architectures\n",
        "# ==============================================================================\n",
        "print(\"\\nDefining Model Architectures...\")\n",
        "\n",
        "# --- Simple CNN ---\n",
        "class SimpleCNN(nn.Module):\n",
        "    \"\"\"A basic CNN architecture.\"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 4 * 4, 512),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# --- ResNet Components ---\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"Defines a 3x3 convolution with padding.\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "        padding=1, bias=False\n",
        "    )\n",
        "\n",
        "class BasicBlock(nn.Module): # Used in ResNet18\n",
        "    \"\"\"Standard Basic Residual Block for ResNet.\"\"\"\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    in_planes, self.expansion * planes,\n",
        "                    kernel_size=1, stride=stride, bias=False\n",
        "                ),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        # Apply operations sequentially using functional ReLU\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out) # Functional ReLU\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.shortcut(identity)\n",
        "        out = F.relu(out) # Functional ReLU\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module): # Base class for ResNet architectures\n",
        "    \"\"\"Generic ResNet base class.\"\"\"\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            3, self.in_planes, kernel_size=3, stride=1, padding=1, bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
        "        # Residual layers are defined using _make_layer\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        # Classifier head\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "        # Initialize weights after defining layers\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        \"\"\"Builds a ResNet layer composed of residual blocks.\"\"\"\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for current_stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, current_stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "         \"\"\"Initializes model weights.\"\"\"\n",
        "         for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(\n",
        "                    m.weight, mode='fan_out', nonlinearity='relu'\n",
        "                )\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x) # Use functional ReLU\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def ResNet18(num_classes=10):\n",
        "    \"\"\"Constructs a ResNet-18 model for CIFAR-10.\"\"\"\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
        "\n",
        "\n",
        "# --- ResNet9 Implementation (Using Functional ReLU) ---\n",
        "def conv_bn(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "    \"\"\"Helper sequence: Convolution -> Batch Normalization.\"\"\"\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size=kernel_size,\n",
        "            stride=stride, padding=padding, bias=False\n",
        "        ),\n",
        "        nn.BatchNorm2d(out_channels)\n",
        "    )\n",
        "\n",
        "class ResNet9Block(nn.Module):\n",
        "    \"\"\"Residual block specific to the ResNet9 architecture used here.\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = conv_bn(channels, channels)\n",
        "        self.conv2 = conv_bn(channels, channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        # Apply functional ReLU *after* each Conv-BN sequence\n",
        "        out = F.relu(self.conv1(x))\n",
        "        out = self.conv2(out)\n",
        "        # Add residual connection *before* the final ReLU of the block\n",
        "        out += residual\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet9(nn.Module):\n",
        "    \"\"\"ResNet9 architecture often used for faster training on CIFAR-10.\"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        # Initial Convolution: 3 -> 64 channels\n",
        "        self.prep = conv_bn(3, 64)\n",
        "\n",
        "        # Layer 1: Conv (64->128), Pool, ResBlock\n",
        "        self.layer1_conv = conv_bn(64, 128)\n",
        "        self.layer1_pool = nn.MaxPool2d(2) # 32x32 -> 16x16\n",
        "        self.layer1_res = ResNet9Block(128)\n",
        "\n",
        "        # Layer 2: Conv (128->256), Pool, ResBlock\n",
        "        self.layer2_conv = conv_bn(128, 256)\n",
        "        self.layer2_pool = nn.MaxPool2d(2) # 16x16 -> 8x8\n",
        "        self.layer2_res = ResNet9Block(256)\n",
        "\n",
        "        # Layer 3: Conv (256->512), Pool, ResBlock\n",
        "        self.layer3_conv = conv_bn(256, 512)\n",
        "        self.layer3_pool = nn.MaxPool2d(2) # 8x8 -> 4x4\n",
        "        self.layer3_res = ResNet9Block(512)\n",
        "\n",
        "        # Classifier Head\n",
        "        self.pool = nn.MaxPool2d(4) # Pool features to 1x1\n",
        "        self.flat = nn.Flatten()\n",
        "        self.fc = nn.Linear(512, num_classes) # Final linear layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.prep(x)) # ReLU after initial Conv-BN\n",
        "\n",
        "        x = self.layer1_conv(x)\n",
        "        x = self.layer1_pool(x)\n",
        "        x = F.relu(x) # ReLU after Conv-Pool\n",
        "        x = self.layer1_res(x) # Residual block includes internal ReLUs\n",
        "\n",
        "        x = self.layer2_conv(x)\n",
        "        x = self.layer2_pool(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.layer2_res(x)\n",
        "\n",
        "        x = self.layer3_conv(x)\n",
        "        x = self.layer3_pool(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.layer3_res(x)\n",
        "\n",
        "        x = self.pool(x)\n",
        "        x = self.flat(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "print(\"Model definitions complete.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. Training and Evaluation Loop\n",
        "# ==============================================================================\n",
        "print(\"\\nDefining Training and Evaluation Function...\")\n",
        "\n",
        "def train_eval_model(\n",
        "    loss_fn_instance: nn.Module,\n",
        "    loss_name: str,\n",
        "    noise_type: str = 'symmetric',\n",
        "    eta: float = 0.2,\n",
        "    epochs: int = EPOCHS,\n",
        "    lr: float = LEARNING_RATE,\n",
        "    batch_size: int = BATCH_SIZE,\n",
        "    seed: int = SEED\n",
        ") -> float:\n",
        "    \"\"\"Trains and evaluates a model for one experimental configuration.\"\"\"\n",
        "    print(\n",
        "        f\"\\n--- Training Start: {loss_name} | \"\n",
        "        f\"Noise: {noise_type}@{eta*100:.0f}% | Epochs: {epochs} ---\"\n",
        "    )\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    noisy_trainset = NoisyCIFAR10(\n",
        "        train_data_clean, noise_type=noise_type, eta=eta,\n",
        "        num_classes=num_classes, transform=transform_train, seed=seed\n",
        "    )\n",
        "    trainloader = DataLoader(\n",
        "        noisy_trainset, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=2, pin_memory=(device.type == 'cuda'), drop_last=True\n",
        "    )\n",
        "    testloader = DataLoader(\n",
        "        test_data, batch_size=batch_size * 2, shuffle=False,\n",
        "        num_workers=2, pin_memory=(device.type == 'cuda')\n",
        "    )\n",
        "\n",
        "    # --- Model Selection ---\n",
        "    if USE_SIMPLE_CNN:\n",
        "        model = SimpleCNN(num_classes=num_classes).to(device)\n",
        "    elif USE_RESNET9:\n",
        "        model = ResNet9(num_classes=num_classes).to(device)\n",
        "    else:\n",
        "        model = ResNet18(num_classes=num_classes).to(device)\n",
        "    print(f\"Using Model: {type(model).__name__}\")\n",
        "\n",
        "    # --- Optimizer, Scheduler, Loss ---\n",
        "    optimizer = optim.SGD(\n",
        "        model.parameters(), lr=lr, momentum=0.9, weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "    criterion = loss_fn_instance.to(device)\n",
        "\n",
        "    best_test_acc = 0.0\n",
        "    start_time = time.time()\n",
        "    print(\n",
        "        f\"Optimizer: SGD(lr={lr:.1e}, momentum=0.9, wd={WEIGHT_DECAY:.1e})\"\n",
        "    )\n",
        "    print(f\"Scheduler: CosineAnnealingLR(T_max={epochs})\")\n",
        "\n",
        "    # --- Training Loop ---\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "        num_batches = len(trainloader)\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            if torch.isnan(loss) or torch.isinf(loss):\n",
        "                print(\n",
        "                    f\"WARN - E{epoch+1:02d} B{i+1}/{num_batches}: \"\n",
        "                    f\"NaN/Inf loss ({loss.item():.4f})! Skipping batch.\"\n",
        "                )\n",
        "                continue\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            with torch.no_grad():\n",
        "                 _, predicted_train = torch.max(outputs.data, 1)\n",
        "                 total_train += labels.size(0)\n",
        "                 correct_train += (predicted_train == labels).sum().item()\n",
        "\n",
        "        # --- Evaluation Step ---\n",
        "        model.eval()\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in testloader:\n",
        "                 inputs, labels = inputs.to(device), labels.to(device)\n",
        "                 outputs = model(inputs)\n",
        "                 _, predicted = torch.max(outputs.data, 1)\n",
        "                 total_test += labels.size(0)\n",
        "                 correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "        # --- Log Epoch Metrics ---\n",
        "        epoch_train_loss = running_loss / num_batches if num_batches > 0 else 0\n",
        "        epoch_train_acc = 100.0 * correct_train / total_train if total_train > 0 else 0\n",
        "        epoch_test_acc = 100.0 * correct_test / total_test if total_test > 0 else 0\n",
        "        best_test_acc = max(best_test_acc, epoch_test_acc)\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "        print(\n",
        "            f\"E {epoch+1:02d}/{epochs} | L:{epoch_train_loss:.4f} | \"\n",
        "            f\"TrA:{epoch_train_acc:6.2f}% | TeA:{epoch_test_acc:6.2f}% \"\n",
        "            f\"(B:{best_test_acc:6.2f}%) | LR:{current_lr:.6f}\"\n",
        "        )\n",
        "        scheduler.step()\n",
        "\n",
        "    # --- End of Training ---\n",
        "    end_time = time.time()\n",
        "    elapsed_time_min = (end_time - start_time) / 60\n",
        "    print(\n",
        "        f\"--- Training End: {loss_name} (Eta:{eta:.2f}). \"\n",
        "        f\"Best Test Acc: {best_test_acc:.2f}%. \"\n",
        "        f\"Duration: {elapsed_time_min:.2f} min ---\"\n",
        "    )\n",
        "    return best_test_acc\n",
        "\n",
        "print(\"Training function defined.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. Experiment Setup\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*20 + \" Experiment Configuration \" + \"=\"*20)\n",
        "\n",
        "all_losses = {\n",
        "    'CE': CrossEntropyLoss(),\n",
        "    'FL (g=1)': FocalLoss(gamma=1.0),\n",
        "    'FL (g=2)': FocalLoss(gamma=2.0),\n",
        "    'NCE (s=1)': NormalizedCrossEntropy(num_classes=num_classes, scale=1.0),\n",
        "    'NFL (g=1,s=1)': NormalizedFocalLoss(num_classes, alpha=1.0, gamma=1.0, scale=1.0),\n",
        "    'NFL (g=2,s=1)': NormalizedFocalLoss(num_classes, alpha=1.0, gamma=2.0, scale=1.0),\n",
        "    'MAE': MeanAbsoluteErrorLoss(num_classes=num_classes),\n",
        "    'RCE': ReverseCrossEntropy(num_classes=num_classes),\n",
        "    'APL (NCE+MAE)': APL(NormalizedCrossEntropy(num_classes, scale=1.0), MeanAbsoluteErrorLoss(num_classes), alpha=1.0, beta=1.0),\n",
        "    'APL (NCE+RCE)': APL(NormalizedCrossEntropy(num_classes, scale=1.0), ReverseCrossEntropy(num_classes), alpha=1.0, beta=0.5),\n",
        "    'APL (NFLg1+MAE)': APL(NormalizedFocalLoss(num_classes, gamma=1.0, scale=1.0), MeanAbsoluteErrorLoss(num_classes), alpha=1.0, beta=1.0),\n",
        "}\n",
        "\n",
        "if LOSSES_TO_RUN is None:\n",
        "    losses_to_test = all_losses\n",
        "    print(\"Running ALL defined loss functions.\")\n",
        "else:\n",
        "    losses_to_test = {}\n",
        "    for name in LOSSES_TO_RUN:\n",
        "         if name in all_losses: losses_to_test[name] = all_losses[name]\n",
        "         else: print(f\"Warning: Loss '{name}' not found. Skipping.\")\n",
        "    if not losses_to_test: print(f\"FATAL: No valid losses selected.\"); exit(1)\n",
        "    print(f\"Running SELECTED losses: {list(losses_to_test.keys())}\")\n",
        "\n",
        "model_name_str = \"ResNet9\" if USE_RESNET9 else (\"SimpleCNN\" if USE_SIMPLE_CNN else \"ResNet18\")\n",
        "print(f\"Model Architecture: {model_name_str}\")\n",
        "print(f\"Epochs per run: {EPOCHS}\")\n",
        "print(f\"Symmetric Noise Rates: {SYMMETRIC_NOISE_RATES}\")\n",
        "if RUN_ASYMMETRIC: print(f\"Asymmetric Noise Rates: {ASYMMETRIC_NOISE_RATES}\")\n",
        "print(f\"Saving Results: {SAVE_RESULTS}\" + (f\" to {RESULTS_DIR}\" if SAVE_RESULTS else \"\"))\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. Run Experiments\n",
        "# ==============================================================================\n",
        "print(\"\\n *** Starting Experiment Execution ***\\n\")\n",
        "results_symmetric = {}\n",
        "results_asymmetric = {}\n",
        "\n",
        "if SAVE_RESULTS:\n",
        "    sym_file = os.path.join(RESULTS_DIR, 'results_symmetric.pth')\n",
        "    asym_file = os.path.join(RESULTS_DIR, 'results_asymmetric.pth')\n",
        "    if os.path.exists(sym_file):\n",
        "        try: results_symmetric = torch.load(sym_file); print(f\"Loaded sym results from {sym_file}\")\n",
        "        except Exception as e: print(f\"WARN: Err load sym:{e}\"); results_symmetric = {}\n",
        "    if os.path.exists(asym_file):\n",
        "        try: results_asymmetric = torch.load(asym_file); print(f\"Loaded asym results from {asym_file}\")\n",
        "        except Exception as e: print(f\"WARN: Err load asym:{e}\"); results_asymmetric = {}\n",
        "\n",
        "def save_experiment_results():\n",
        "    if SAVE_RESULTS:\n",
        "        try:\n",
        "            print(f\"\\nCheckpoint: Saving results...\"); os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "            sym_file=os.path.join(RESULTS_DIR, 'results_symmetric.pth'); asym_file=os.path.join(RESULTS_DIR, 'results_asymmetric.pth')\n",
        "            torch.save(results_symmetric, sym_file)\n",
        "            if RUN_ASYMMETRIC: torch.save(results_asymmetric, asym_file)\n",
        "            print(\"Checkpoint saved.\")\n",
        "        except Exception as e: print(f\"WARN: Err saving results checkpoint: {e}\")\n",
        "\n",
        "try:\n",
        "    if RUN_SYMMETRIC:\n",
        "        print(f\"\\n=== Processing Symmetric Noise Experiments ===\")\n",
        "        for loss_name, loss_instance in losses_to_test.items():\n",
        "            current_results = results_symmetric.get(loss_name)\n",
        "            if isinstance(current_results, list) and len(current_results) == len(SYMMETRIC_NOISE_RATES):\n",
        "                print(f\" -> Skip {loss_name}(Sym)-Exist\"); continue\n",
        "            print(f\" --> Running {loss_name}(Sym)...\"); noise_accuracies = []\n",
        "            for eta in SYMMETRIC_NOISE_RATES:\n",
        "                acc = train_eval_model(loss_instance, loss_name, 'symmetric', eta) # Pass correct args\n",
        "                noise_accuracies.append(acc)\n",
        "            results_symmetric[loss_name] = noise_accuracies; save_experiment_results()\n",
        "\n",
        "    if RUN_ASYMMETRIC:\n",
        "        print(f\"\\n=== Processing Asymmetric Noise Experiments ===\")\n",
        "        for loss_name, loss_instance in losses_to_test.items():\n",
        "             current_results = results_asymmetric.get(loss_name)\n",
        "             if isinstance(current_results, list) and len(current_results) == len(ASYMMETRIC_NOISE_RATES):\n",
        "                 print(f\" -> Skip {loss_name}(Asym)-Exist\"); continue\n",
        "             print(f\" --> Running {loss_name}(Asym)...\"); noise_accuracies = []\n",
        "             for eta in ASYMMETRIC_NOISE_RATES:\n",
        "                 acc = train_eval_model(loss_instance, loss_name, 'asymmetric', eta) # Pass correct args\n",
        "                 noise_accuracies.append(acc)\n",
        "             results_asymmetric[loss_name] = noise_accuracies; save_experiment_results()\n",
        "\n",
        "finally:\n",
        "    print(\"\\nExperiment execution phase finished or interrupted.\")\n",
        "    save_experiment_results()\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. Plot Results\n",
        "# ==============================================================================\n",
        "print(\"\\nGenerating Plots...\")\n",
        "loss_styles={'CE':{'marker':'o','color':'C0','linestyle':'-'},'FL (g=1)':{'marker':'P','color':'C1','linestyle':'-'},'FL (g=2)':{'marker':'X','color':'C2','linestyle':'-'},'NCE (s=1)':{'marker':'s','color':'C3','linestyle':'-'},'NFL (g=1,s=1)':{'marker':'d','color':'C4','linestyle':'-'},'NFL (g=2,s=1)':{'marker':'D','color':'C5','linestyle':'-'},'MAE':{'marker':'^','color':'C6','linestyle':'-'},'RCE':{'marker':'v','color':'C7','linestyle':'-'},'APL (NCE+MAE)':{'marker':'s','color':'C3','linestyle':'--'},'APL (NCE+RCE)':{'marker':'v','color':'C7','linestyle':'--'},'APL (NFLg1+MAE)':{'marker':'d','color':'C4','linestyle':'--'},}\n",
        "default_style={'marker':'*','color':'black','linestyle':':'}\n",
        "def plot_results(results, noise_rates, title_suffix):\n",
        "    valid_losses=[name for name,data in results.items() if data and isinstance(data,list) and len(data)==len(noise_rates)]\n",
        "    if not valid_losses: print(f\"No plottable results for {title_suffix}.\"); return\n",
        "    plt.figure(figsize=(12,8)); model_name_plot=\"ResNet9\" if USE_RESNET9 else(\"SimpleCNN\" if USE_SIMPLE_CNN else \"ResNet18\")\n",
        "    for loss_name in valid_losses:\n",
        "        acc_list=results[loss_name]; style=loss_styles.get(loss_name,default_style)\n",
        "        plt.plot(noise_rates,acc_list,label=loss_name,marker=style.get('marker','*'),color=style.get('color'),linestyle=style.get('linestyle',':'))\n",
        "    plt.xlabel('Noise Rate (η)'); plt.ylabel('Best Test Accuracy (%)'); plt.title(f'Model Perf Under {title_suffix} Noise\\n(E:{EPOCHS}, M:{model_name_plot})'); num_entries=len(valid_losses); num_cols=1 if num_entries<=6 else math.ceil(num_entries/6); plt.legend(loc='best',fontsize='small',ncol=num_cols); plt.grid(True,linestyle='--',alpha=0.7); plt.ylim(bottom=0,top=100); plt.xticks(noise_rates); plt.tight_layout(); plt.show()\n",
        "if RUN_SYMMETRIC: plot_results(results_symmetric,SYMMETRIC_NOISE_RATES,\"Symmetric\")\n",
        "if RUN_ASYMMETRIC: plot_results(results_asymmetric,ASYMMETRIC_NOISE_RATES,\"Asymmetric\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 8. Print Final Results Tables\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Final Results Tables ---\")\n",
        "def print_results_table(results_dict,rates,name):\n",
        "    print(f\"\\n{name} Noise Results (Best Test Accuracy %):\"); valid_results={k:v for k,v in results_dict.items() if isinstance(v,list) and len(v)==len(rates)}\n",
        "    if not valid_results: print(\"  No valid results.\"); return\n",
        "    try:\n",
        "        df=pd.DataFrame(valid_results); df=df.reindex(index=rates); df.index.name='Noise Rate (η)'\n",
        "        ordered_cols=[col for col in all_losses if col in df.columns]; remaining_cols=[col for col in df.columns if col not in ordered_cols]; final_cols=ordered_cols+remaining_cols; df=df[final_cols]\n",
        "        print(df.to_string(float_format=\"%.2f\"))\n",
        "    except Exception as e: print(f\"  Table Error:{e}\\n  Raw Data:{results_dict}\")\n",
        "if RUN_SYMMETRIC: print_results_table(results_symmetric,SYMMETRIC_NOISE_RATES,\"Symmetric\")\n",
        "if RUN_ASYMMETRIC: print_results_table(results_asymmetric,ASYMMETRIC_NOISE_RATES,\"Asymmetric\")\n",
        "\n",
        "print(\"\\n--- Experiment Run Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RffwOv5cnqjE",
        "outputId": "2df92daa-4751-43ab-e2d3-45918f181ee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.6.0+cu124\n",
            "Torchvision Version: 0.21.0+cu124\n",
            "Running in Colab: True\n",
            "\n",
            "Setting up data transformations...\n",
            "Loading CIFAR-10 dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:12<00:00, 13.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIFAR-10 loaded: 50000 train, 10000 test images.\n",
            "\n",
            "Defining Loss Functions...\n",
            "\n",
            "Defining Model Architectures...\n",
            "Model definitions complete.\n",
            "\n",
            "Defining Training and Evaluation Function...\n",
            "Training function defined.\n",
            "\n",
            "==================== Experiment Configuration ====================\n",
            "Running SELECTED losses: ['CE', 'NCE (s=1)', 'APL (NCE+MAE)', 'APL (NCE+RCE)']\n",
            "Model Architecture: ResNet9\n",
            "Epochs per run: 50\n",
            "Symmetric Noise Rates: [0.0, 0.2, 0.4, 0.6, 0.8]\n",
            "Saving Results: False\n",
            "============================================================\n",
            "\n",
            " *** Starting Experiment Execution ***\n",
            "\n",
            "\n",
            "=== Processing Symmetric Noise Experiments ===\n",
            " --> Running CE(Sym)...\n",
            "\n",
            "--- Training Start: CE | Noise: symmetric@0% | Epochs: 50 ---\n",
            "Using device: cuda\n",
            "Using Model: ResNet9\n",
            "Optimizer: SGD(lr=1.0e-01, momentum=0.9, wd=5.0e-04)\n",
            "Scheduler: CosineAnnealingLR(T_max=50)\n",
            "E 01/50 | L:3.5731 | TrA: 15.99% | TeA: 19.77% (B: 19.77%) | LR:0.100000\n",
            "E 02/50 | L:2.0438 | TrA: 20.06% | TeA: 22.12% (B: 22.12%) | LR:0.099901\n",
            "E 03/50 | L:1.9187 | TrA: 23.83% | TeA: 22.89% (B: 22.89%) | LR:0.099606\n",
            "E 04/50 | L:1.7899 | TrA: 29.45% | TeA: 27.69% (B: 27.69%) | LR:0.099114\n",
            "E 05/50 | L:1.5672 | TrA: 40.21% | TeA: 46.93% (B: 46.93%) | LR:0.098429\n",
            "E 06/50 | L:1.3164 | TrA: 52.07% | TeA: 57.00% (B: 57.00%) | LR:0.097553\n",
            "E 07/50 | L:1.0676 | TrA: 62.02% | TeA: 59.29% (B: 59.29%) | LR:0.096489\n",
            "E 08/50 | L:0.8740 | TrA: 69.30% | TeA: 59.25% (B: 59.29%) | LR:0.095241\n",
            "E 09/50 | L:0.7578 | TrA: 73.52% | TeA: 69.96% (B: 69.96%) | LR:0.093815\n",
            "E 10/50 | L:0.6779 | TrA: 76.59% | TeA: 72.56% (B: 72.56%) | LR:0.092216\n",
            "E 11/50 | L:0.6185 | TrA: 78.75% | TeA: 76.37% (B: 76.37%) | LR:0.090451\n",
            "E 12/50 | L:0.5802 | TrA: 80.06% | TeA: 76.42% (B: 76.42%) | LR:0.088526\n",
            "E 13/50 | L:0.5472 | TrA: 81.32% | TeA: 70.63% (B: 76.42%) | LR:0.086448\n",
            "E 14/50 | L:0.5184 | TrA: 82.41% | TeA: 79.98% (B: 79.98%) | LR:0.084227\n",
            "E 15/50 | L:0.4882 | TrA: 83.33% | TeA: 79.62% (B: 79.98%) | LR:0.081871\n",
            "E 16/50 | L:0.4699 | TrA: 83.79% | TeA: 79.82% (B: 79.98%) | LR:0.079389\n",
            "E 17/50 | L:0.4492 | TrA: 84.68% | TeA: 81.46% (B: 81.46%) | LR:0.076791\n",
            "E 18/50 | L:0.4368 | TrA: 85.09% | TeA: 79.81% (B: 81.46%) | LR:0.074088\n",
            "E 19/50 | L:0.4107 | TrA: 85.80% | TeA: 79.92% (B: 81.46%) | LR:0.071289\n",
            "E 20/50 | L:0.3961 | TrA: 86.27% | TeA: 82.73% (B: 82.73%) | LR:0.068406\n",
            "E 21/50 | L:0.3808 | TrA: 86.91% | TeA: 84.03% (B: 84.03%) | LR:0.065451\n",
            "E 22/50 | L:0.3642 | TrA: 87.45% | TeA: 81.63% (B: 84.03%) | LR:0.062434\n",
            "E 23/50 | L:0.3508 | TrA: 87.89% | TeA: 84.45% (B: 84.45%) | LR:0.059369\n",
            "E 24/50 | L:0.3320 | TrA: 88.56% | TeA: 84.34% (B: 84.45%) | LR:0.056267\n",
            "E 25/50 | L:0.3224 | TrA: 88.89% | TeA: 83.38% (B: 84.45%) | LR:0.053140\n",
            "E 26/50 | L:0.3010 | TrA: 89.62% | TeA: 85.88% (B: 85.88%) | LR:0.050000\n",
            "E 27/50 | L:0.2886 | TrA: 89.89% | TeA: 84.00% (B: 85.88%) | LR:0.046860\n",
            "E 28/50 | L:0.2728 | TrA: 90.49% | TeA: 83.10% (B: 85.88%) | LR:0.043733\n",
            "E 29/50 | L:0.2484 | TrA: 91.36% | TeA: 87.04% (B: 87.04%) | LR:0.040631\n",
            "E 30/50 | L:0.2414 | TrA: 91.46% | TeA: 85.89% (B: 87.04%) | LR:0.037566\n",
            "E 31/50 | L:0.2187 | TrA: 92.50% | TeA: 86.01% (B: 87.04%) | LR:0.034549\n",
            "E 32/50 | L:0.2099 | TrA: 92.61% | TeA: 89.08% (B: 89.08%) | LR:0.031594\n",
            "E 33/50 | L:0.1850 | TrA: 93.54% | TeA: 88.87% (B: 89.08%) | LR:0.028711\n",
            "E 34/50 | L:0.1753 | TrA: 94.05% | TeA: 88.26% (B: 89.08%) | LR:0.025912\n",
            "E 35/50 | L:0.1568 | TrA: 94.54% | TeA: 88.40% (B: 89.08%) | LR:0.023209\n",
            "E 36/50 | L:0.1434 | TrA: 94.94% | TeA: 88.89% (B: 89.08%) | LR:0.020611\n",
            "E 37/50 | L:0.1183 | TrA: 95.94% | TeA: 89.45% (B: 89.45%) | LR:0.018129\n",
            "E 38/50 | L:0.1055 | TrA: 96.30% | TeA: 90.21% (B: 90.21%) | LR:0.015773\n",
            "E 39/50 | L:0.0881 | TrA: 96.95% | TeA: 90.85% (B: 90.85%) | LR:0.013552\n",
            "E 40/50 | L:0.0747 | TrA: 97.48% | TeA: 91.18% (B: 91.18%) | LR:0.011474\n",
            "E 41/50 | L:0.0603 | TrA: 97.96% | TeA: 91.32% (B: 91.32%) | LR:0.009549\n",
            "E 42/50 | L:0.0489 | TrA: 98.43% | TeA: 91.70% (B: 91.70%) | LR:0.007784\n",
            "E 43/50 | L:0.0392 | TrA: 98.76% | TeA: 91.89% (B: 91.89%) | LR:0.006185\n",
            "E 44/50 | L:0.0314 | TrA: 98.98% | TeA: 92.25% (B: 92.25%) | LR:0.004759\n",
            "E 45/50 | L:0.0259 | TrA: 99.25% | TeA: 92.23% (B: 92.25%) | LR:0.003511\n",
            "E 46/50 | L:0.0221 | TrA: 99.37% | TeA: 92.30% (B: 92.30%) | LR:0.002447\n",
            "E 47/50 | L:0.0189 | TrA: 99.50% | TeA: 92.44% (B: 92.44%) | LR:0.001571\n",
            "E 48/50 | L:0.0186 | TrA: 99.50% | TeA: 92.35% (B: 92.44%) | LR:0.000886\n",
            "E 49/50 | L:0.0166 | TrA: 99.58% | TeA: 92.43% (B: 92.44%) | LR:0.000394\n",
            "E 50/50 | L:0.0159 | TrA: 99.63% | TeA: 92.44% (B: 92.44%) | LR:0.000099\n",
            "--- Training End: CE (Eta:0.00). Best Test Acc: 92.44%. Duration: 23.90 min ---\n",
            "\n",
            "--- Training Start: CE | Noise: symmetric@20% | Epochs: 50 ---\n",
            "Using device: cuda\n",
            "Using Model: ResNet9\n",
            "Optimizer: SGD(lr=1.0e-01, momentum=0.9, wd=5.0e-04)\n",
            "Scheduler: CosineAnnealingLR(T_max=50)\n",
            "E 01/50 | L:3.5238 | TrA: 12.97% | TeA: 17.31% (B: 17.31%) | LR:0.100000\n",
            "E 02/50 | L:2.1899 | TrA: 17.81% | TeA: 24.97% (B: 24.97%) | LR:0.099901\n",
            "E 03/50 | L:2.1106 | TrA: 21.22% | TeA: 26.35% (B: 26.35%) | LR:0.099606\n",
            "E 04/50 | L:2.0507 | TrA: 24.64% | TeA: 33.26% (B: 33.26%) | LR:0.099114\n",
            "E 05/50 | L:1.9732 | TrA: 29.54% | TeA: 41.29% (B: 41.29%) | LR:0.098429\n",
            "E 06/50 | L:1.8752 | TrA: 35.02% | TeA: 43.71% (B: 43.71%) | LR:0.097553\n",
            "E 07/50 | L:1.7665 | TrA: 41.52% | TeA: 53.67% (B: 53.67%) | LR:0.096489\n",
            "E 08/50 | L:1.6546 | TrA: 47.89% | TeA: 47.63% (B: 53.67%) | LR:0.095241\n",
            "E 09/50 | L:1.5750 | TrA: 51.94% | TeA: 60.11% (B: 60.11%) | LR:0.093815\n",
            "E 10/50 | L:1.5014 | TrA: 55.46% | TeA: 64.84% (B: 64.84%) | LR:0.092216\n",
            "E 11/50 | L:1.4526 | TrA: 57.93% | TeA: 63.87% (B: 64.84%) | LR:0.090451\n",
            "E 12/50 | L:1.4184 | TrA: 59.56% | TeA: 69.20% (B: 69.20%) | LR:0.088526\n",
            "E 13/50 | L:1.3946 | TrA: 60.62% | TeA: 67.68% (B: 69.20%) | LR:0.086448\n",
            "E 14/50 | L:1.3691 | TrA: 61.50% | TeA: 74.49% (B: 74.49%) | LR:0.084227\n",
            "E 15/50 | L:1.3481 | TrA: 62.77% | TeA: 73.10% (B: 74.49%) | LR:0.081871\n",
            "E 16/50 | L:1.3310 | TrA: 63.30% | TeA: 72.89% (B: 74.49%) | LR:0.079389\n",
            "E 17/50 | L:1.3175 | TrA: 63.83% | TeA: 72.41% (B: 74.49%) | LR:0.076791\n",
            "E 18/50 | L:1.3031 | TrA: 64.45% | TeA: 73.30% (B: 74.49%) | LR:0.074088\n",
            "E 19/50 | L:1.2908 | TrA: 65.11% | TeA: 73.06% (B: 74.49%) | LR:0.071289\n",
            "E 20/50 | L:1.2742 | TrA: 65.89% | TeA: 75.54% (B: 75.54%) | LR:0.068406\n",
            "E 21/50 | L:1.2592 | TrA: 66.39% | TeA: 70.59% (B: 75.54%) | LR:0.065451\n",
            "E 22/50 | L:1.2533 | TrA: 66.64% | TeA: 75.85% (B: 75.85%) | LR:0.062434\n",
            "E 23/50 | L:1.2332 | TrA: 67.25% | TeA: 74.96% (B: 75.85%) | LR:0.059369\n",
            "E 24/50 | L:1.2208 | TrA: 67.80% | TeA: 78.99% (B: 78.99%) | LR:0.056267\n",
            "E 25/50 | L:1.2113 | TrA: 68.08% | TeA: 76.00% (B: 78.99%) | LR:0.053140\n",
            "E 26/50 | L:1.1982 | TrA: 68.76% | TeA: 81.13% (B: 81.13%) | LR:0.050000\n",
            "E 27/50 | L:1.1835 | TrA: 69.31% | TeA: 77.52% (B: 81.13%) | LR:0.046860\n",
            "E 28/50 | L:1.1725 | TrA: 69.52% | TeA: 75.98% (B: 81.13%) | LR:0.043733\n",
            "E 29/50 | L:1.1550 | TrA: 70.34% | TeA: 82.39% (B: 82.39%) | LR:0.040631\n",
            "E 30/50 | L:1.1389 | TrA: 70.56% | TeA: 81.77% (B: 82.39%) | LR:0.037566\n",
            "E 31/50 | L:1.1269 | TrA: 71.16% | TeA: 82.01% (B: 82.39%) | LR:0.034549\n",
            "E 32/50 | L:1.1076 | TrA: 71.62% | TeA: 81.60% (B: 82.39%) | LR:0.031594\n",
            "E 33/50 | L:1.0947 | TrA: 72.06% | TeA: 85.28% (B: 85.28%) | LR:0.028711\n",
            "E 34/50 | L:1.0731 | TrA: 72.78% | TeA: 84.18% (B: 85.28%) | LR:0.025912\n",
            "E 35/50 | L:1.0576 | TrA: 73.30% | TeA: 83.62% (B: 85.28%) | LR:0.023209\n",
            "E 36/50 | L:1.0411 | TrA: 73.73% | TeA: 83.40% (B: 85.28%) | LR:0.020611\n",
            "E 37/50 | L:1.0154 | TrA: 74.28% | TeA: 85.87% (B: 85.87%) | LR:0.018129\n",
            "E 38/50 | L:0.9923 | TrA: 75.02% | TeA: 86.19% (B: 86.19%) | LR:0.015773\n",
            "E 39/50 | L:0.9674 | TrA: 75.64% | TeA: 85.78% (B: 86.19%) | LR:0.013552\n",
            "E 40/50 | L:0.9412 | TrA: 76.08% | TeA: 84.04% (B: 86.19%) | LR:0.011474\n",
            "E 41/50 | L:0.9113 | TrA: 76.65% | TeA: 85.96% (B: 86.19%) | LR:0.009549\n",
            "E 42/50 | L:0.8776 | TrA: 77.51% | TeA: 85.68% (B: 86.19%) | LR:0.007784\n",
            "E 43/50 | L:0.8399 | TrA: 78.10% | TeA: 85.28% (B: 86.19%) | LR:0.006185\n",
            "E 44/50 | L:0.7997 | TrA: 78.94% | TeA: 85.65% (B: 86.19%) | LR:0.004759\n",
            "E 45/50 | L:0.7584 | TrA: 79.98% | TeA: 84.46% (B: 86.19%) | LR:0.003511\n",
            "E 46/50 | L:0.7163 | TrA: 80.70% | TeA: 85.23% (B: 86.19%) | LR:0.002447\n",
            "E 47/50 | L:0.6731 | TrA: 81.53% | TeA: 84.38% (B: 86.19%) | LR:0.001571\n",
            "E 48/50 | L:0.6359 | TrA: 82.32% | TeA: 84.56% (B: 86.19%) | LR:0.000886\n",
            "E 49/50 | L:0.6124 | TrA: 83.02% | TeA: 84.27% (B: 86.19%) | LR:0.000394\n",
            "E 50/50 | L:0.5962 | TrA: 83.36% | TeA: 84.05% (B: 86.19%) | LR:0.000099\n",
            "--- Training End: CE (Eta:0.20). Best Test Acc: 86.19%. Duration: 23.99 min ---\n",
            "\n",
            "--- Training Start: CE | Noise: symmetric@40% | Epochs: 50 ---\n",
            "Using device: cuda\n",
            "Using Model: ResNet9\n",
            "Optimizer: SGD(lr=1.0e-01, momentum=0.9, wd=5.0e-04)\n",
            "Scheduler: CosineAnnealingLR(T_max=50)\n",
            "E 01/50 | L:3.6365 | TrA: 11.16% | TeA: 12.98% (B: 12.98%) | LR:0.100000\n",
            "E 02/50 | L:2.2751 | TrA: 13.00% | TeA: 18.34% (B: 18.34%) | LR:0.099901\n",
            "E 03/50 | L:2.2425 | TrA: 15.27% | TeA: 21.93% (B: 21.93%) | LR:0.099606\n",
            "E 04/50 | L:2.2158 | TrA: 16.02% | TeA: 22.14% (B: 22.14%) | LR:0.099114\n",
            "E 05/50 | L:2.2051 | TrA: 16.86% | TeA: 25.47% (B: 25.47%) | LR:0.098429\n",
            "E 06/50 | L:2.1818 | TrA: 19.29% | TeA: 29.70% (B: 29.70%) | LR:0.097553\n",
            "E 07/50 | L:2.1567 | TrA: 22.16% | TeA: 35.19% (B: 35.19%) | LR:0.096489\n",
            "E 08/50 | L:2.1237 | TrA: 24.58% | TeA: 33.90% (B: 35.19%) | LR:0.095241\n",
            "E 09/50 | L:2.0897 | TrA: 27.61% | TeA: 46.65% (B: 46.65%) | LR:0.093815\n",
            "E 10/50 | L:2.0504 | TrA: 30.71% | TeA: 44.93% (B: 46.65%) | LR:0.092216\n",
            "E 11/50 | L:2.0075 | TrA: 33.72% | TeA: 50.69% (B: 50.69%) | LR:0.090451\n",
            "E 12/50 | L:1.9695 | TrA: 36.42% | TeA: 51.17% (B: 51.17%) | LR:0.088526\n",
            "E 13/50 | L:1.9289 | TrA: 39.24% | TeA: 58.91% (B: 58.91%) | LR:0.086448\n",
            "E 14/50 | L:1.8984 | TrA: 41.37% | TeA: 60.14% (B: 60.14%) | LR:0.084227\n",
            "E 15/50 | L:1.8786 | TrA: 42.60% | TeA: 60.08% (B: 60.14%) | LR:0.081871\n",
            "E 16/50 | L:1.8589 | TrA: 43.76% | TeA: 68.27% (B: 68.27%) | LR:0.079389\n",
            "E 17/50 | L:1.8454 | TrA: 44.41% | TeA: 58.75% (B: 68.27%) | LR:0.076791\n",
            "E 18/50 | L:1.8402 | TrA: 44.71% | TeA: 64.85% (B: 68.27%) | LR:0.074088\n",
            "E 19/50 | L:1.8203 | TrA: 45.82% | TeA: 65.17% (B: 68.27%) | LR:0.071289\n",
            "E 20/50 | L:1.8167 | TrA: 45.83% | TeA: 62.22% (B: 68.27%) | LR:0.068406\n",
            "E 21/50 | L:1.8034 | TrA: 46.54% | TeA: 72.90% (B: 72.90%) | LR:0.065451\n",
            "E 22/50 | L:1.7968 | TrA: 47.14% | TeA: 69.33% (B: 72.90%) | LR:0.062434\n",
            "E 23/50 | L:1.7838 | TrA: 47.71% | TeA: 72.71% (B: 72.90%) | LR:0.059369\n",
            "E 24/50 | L:1.7777 | TrA: 47.55% | TeA: 75.00% (B: 75.00%) | LR:0.056267\n",
            "E 25/50 | L:1.7670 | TrA: 48.53% | TeA: 70.01% (B: 75.00%) | LR:0.053140\n",
            "E 26/50 | L:1.7614 | TrA: 48.77% | TeA: 74.22% (B: 75.00%) | LR:0.050000\n",
            "E 27/50 | L:1.7485 | TrA: 49.28% | TeA: 74.54% (B: 75.00%) | LR:0.046860\n",
            "E 28/50 | L:1.7380 | TrA: 49.93% | TeA: 71.81% (B: 75.00%) | LR:0.043733\n",
            "E 29/50 | L:1.7279 | TrA: 50.20% | TeA: 75.06% (B: 75.06%) | LR:0.040631\n",
            "E 30/50 | L:1.7201 | TrA: 50.59% | TeA: 78.48% (B: 78.48%) | LR:0.037566\n",
            "E 31/50 | L:1.7085 | TrA: 51.13% | TeA: 79.57% (B: 79.57%) | LR:0.034549\n",
            "E 32/50 | L:1.6969 | TrA: 51.53% | TeA: 77.35% (B: 79.57%) | LR:0.031594\n",
            "E 33/50 | L:1.6849 | TrA: 51.96% | TeA: 78.13% (B: 79.57%) | LR:0.028711\n",
            "E 34/50 | L:1.6742 | TrA: 52.44% | TeA: 79.22% (B: 79.57%) | LR:0.025912\n",
            "E 35/50 | L:1.6586 | TrA: 52.99% | TeA: 81.06% (B: 81.06%) | LR:0.023209\n",
            "E 36/50 | L:1.6449 | TrA: 53.58% | TeA: 79.72% (B: 81.06%) | LR:0.020611\n",
            "E 37/50 | L:1.6283 | TrA: 54.11% | TeA: 81.45% (B: 81.45%) | LR:0.018129\n",
            "E 38/50 | L:1.6167 | TrA: 54.34% | TeA: 81.16% (B: 81.45%) | LR:0.015773\n",
            "E 39/50 | L:1.5975 | TrA: 55.03% | TeA: 81.24% (B: 81.45%) | LR:0.013552\n",
            "E 40/50 | L:1.5784 | TrA: 55.36% | TeA: 81.16% (B: 81.45%) | LR:0.011474\n",
            "E 41/50 | L:1.5564 | TrA: 55.97% | TeA: 82.02% (B: 82.02%) | LR:0.009549\n",
            "E 42/50 | L:1.5324 | TrA: 56.69% | TeA: 81.74% (B: 82.02%) | LR:0.007784\n",
            "E 43/50 | L:1.5043 | TrA: 57.36% | TeA: 81.71% (B: 82.02%) | LR:0.006185\n",
            "E 44/50 | L:1.4728 | TrA: 58.17% | TeA: 81.50% (B: 82.02%) | LR:0.004759\n",
            "E 45/50 | L:1.4362 | TrA: 59.06% | TeA: 81.09% (B: 82.02%) | LR:0.003511\n",
            "E 46/50 | L:1.4026 | TrA: 59.98% | TeA: 80.24% (B: 82.02%) | LR:0.002447\n",
            "E 47/50 | L:1.3643 | TrA: 60.78% | TeA: 80.09% (B: 82.02%) | LR:0.001571\n",
            "E 48/50 | L:1.3266 | TrA: 61.61% | TeA: 79.60% (B: 82.02%) | LR:0.000886\n",
            "E 49/50 | L:1.3012 | TrA: 62.43% | TeA: 79.22% (B: 82.02%) | LR:0.000394\n",
            "E 50/50 | L:1.2857 | TrA: 62.55% | TeA: 79.04% (B: 82.02%) | LR:0.000099\n",
            "--- Training End: CE (Eta:0.40). Best Test Acc: 82.02%. Duration: 23.86 min ---\n",
            "\n",
            "--- Training Start: CE | Noise: symmetric@60% | Epochs: 50 ---\n",
            "Using device: cuda\n",
            "Using Model: ResNet9\n",
            "Optimizer: SGD(lr=1.0e-01, momentum=0.9, wd=5.0e-04)\n",
            "Scheduler: CosineAnnealingLR(T_max=50)\n",
            "E 01/50 | L:3.7316 | TrA: 10.19% | TeA: 10.00% (B: 10.00%) | LR:0.100000\n",
            "E 02/50 | L:2.3046 | TrA:  9.86% | TeA: 10.02% (B: 10.02%) | LR:0.099901\n",
            "E 03/50 | L:2.3047 | TrA:  9.91% | TeA:  9.98% (B: 10.02%) | LR:0.099606\n",
            "E 04/50 | L:2.3044 | TrA: 10.07% | TeA:  9.99% (B: 10.02%) | LR:0.099114\n",
            "E 05/50 | L:2.3041 | TrA: 10.21% | TeA: 10.73% (B: 10.73%) | LR:0.098429\n",
            "E 06/50 | L:2.2961 | TrA: 11.39% | TeA: 17.79% (B: 17.79%) | LR:0.097553\n",
            "E 07/50 | L:2.2866 | TrA: 12.60% | TeA: 17.83% (B: 17.83%) | LR:0.096489\n",
            "E 08/50 | L:2.2841 | TrA: 12.46% | TeA: 20.62% (B: 20.62%) | LR:0.095241\n",
            "E 09/50 | L:2.2821 | TrA: 12.69% | TeA: 16.41% (B: 20.62%) | LR:0.093815\n",
            "E 10/50 | L:2.2781 | TrA: 12.85% | TeA: 22.38% (B: 22.38%) | LR:0.092216\n",
            "E 11/50 | L:2.2727 | TrA: 13.81% | TeA: 22.49% (B: 22.49%) | LR:0.090451\n",
            "E 12/50 | L:2.2677 | TrA: 14.72% | TeA: 23.49% (B: 23.49%) | LR:0.088526\n",
            "E 13/50 | L:2.2587 | TrA: 16.10% | TeA: 34.57% (B: 34.57%) | LR:0.086448\n",
            "E 14/50 | L:2.2496 | TrA: 17.09% | TeA: 32.97% (B: 34.57%) | LR:0.084227\n",
            "E 15/50 | L:2.2417 | TrA: 18.40% | TeA: 36.92% (B: 36.92%) | LR:0.081871\n",
            "E 16/50 | L:2.2360 | TrA: 18.96% | TeA: 39.03% (B: 39.03%) | LR:0.079389\n",
            "E 17/50 | L:2.2264 | TrA: 20.14% | TeA: 44.15% (B: 44.15%) | LR:0.076791\n",
            "E 18/50 | L:2.2159 | TrA: 21.51% | TeA: 47.80% (B: 47.80%) | LR:0.074088\n",
            "E 19/50 | L:2.2020 | TrA: 22.88% | TeA: 49.90% (B: 49.90%) | LR:0.071289\n",
            "E 20/50 | L:2.1888 | TrA: 24.20% | TeA: 48.05% (B: 49.90%) | LR:0.068406\n",
            "E 21/50 | L:2.1784 | TrA: 25.31% | TeA: 55.26% (B: 55.26%) | LR:0.065451\n",
            "E 22/50 | L:2.1687 | TrA: 26.17% | TeA: 47.36% (B: 55.26%) | LR:0.062434\n",
            "E 23/50 | L:2.1630 | TrA: 26.75% | TeA: 55.94% (B: 55.94%) | LR:0.059369\n",
            "E 24/50 | L:2.1545 | TrA: 27.42% | TeA: 59.72% (B: 59.72%) | LR:0.056267\n",
            "E 25/50 | L:2.1470 | TrA: 28.02% | TeA: 62.97% (B: 62.97%) | LR:0.053140\n",
            "E 26/50 | L:2.1386 | TrA: 28.70% | TeA: 59.00% (B: 62.97%) | LR:0.050000\n",
            "E 27/50 | L:2.1311 | TrA: 29.33% | TeA: 62.46% (B: 62.97%) | LR:0.046860\n",
            "E 28/50 | L:2.1242 | TrA: 29.83% | TeA: 60.51% (B: 62.97%) | LR:0.043733\n",
            "E 29/50 | L:2.1159 | TrA: 30.48% | TeA: 61.04% (B: 62.97%) | LR:0.040631\n",
            "E 30/50 | L:2.1116 | TrA: 30.71% | TeA: 63.12% (B: 63.12%) | LR:0.037566\n",
            "E 31/50 | L:2.1053 | TrA: 31.11% | TeA: 72.35% (B: 72.35%) | LR:0.034549\n",
            "E 32/50 | L:2.0968 | TrA: 31.67% | TeA: 68.70% (B: 72.35%) | LR:0.031594\n",
            "E 33/50 | L:2.0906 | TrA: 32.00% | TeA: 69.18% (B: 72.35%) | LR:0.028711\n",
            "E 34/50 | L:2.0830 | TrA: 32.44% | TeA: 69.86% (B: 72.35%) | LR:0.025912\n",
            "E 35/50 | L:2.0745 | TrA: 32.94% | TeA: 73.98% (B: 73.98%) | LR:0.023209\n",
            "E 36/50 | L:2.0693 | TrA: 33.00% | TeA: 69.32% (B: 73.98%) | LR:0.020611\n",
            "E 37/50 | L:2.0599 | TrA: 33.66% | TeA: 72.61% (B: 73.98%) | LR:0.018129\n",
            "E 38/50 | L:2.0500 | TrA: 34.13% | TeA: 73.68% (B: 73.98%) | LR:0.015773\n",
            "E 39/50 | L:2.0407 | TrA: 34.43% | TeA: 73.66% (B: 73.98%) | LR:0.013552\n",
            "E 40/50 | L:2.0304 | TrA: 34.91% | TeA: 75.06% (B: 75.06%) | LR:0.011474\n",
            "E 41/50 | L:2.0181 | TrA: 35.36% | TeA: 75.18% (B: 75.18%) | LR:0.009549\n",
            "E 42/50 | L:2.0043 | TrA: 35.81% | TeA: 74.21% (B: 75.18%) | LR:0.007784\n",
            "E 43/50 | L:1.9891 | TrA: 36.46% | TeA: 74.39% (B: 75.18%) | LR:0.006185\n",
            "E 44/50 | L:1.9705 | TrA: 37.04% | TeA: 73.56% (B: 75.18%) | LR:0.004759\n",
            "E 45/50 | L:1.9464 | TrA: 37.84% | TeA: 72.62% (B: 75.18%) | LR:0.003511\n",
            "E 46/50 | L:1.9275 | TrA: 38.49% | TeA: 73.26% (B: 75.18%) | LR:0.002447\n",
            "E 47/50 | L:1.8995 | TrA: 39.12% | TeA: 70.82% (B: 75.18%) | LR:0.001571\n",
            "E 48/50 | L:1.8727 | TrA: 40.20% | TeA: 70.96% (B: 75.18%) | LR:0.000886\n",
            "E 49/50 | L:1.8517 | TrA: 40.90% | TeA: 71.49% (B: 75.18%) | LR:0.000394\n",
            "E 50/50 | L:1.8322 | TrA: 41.57% | TeA: 70.50% (B: 75.18%) | LR:0.000099\n",
            "--- Training End: CE (Eta:0.60). Best Test Acc: 75.18%. Duration: 23.84 min ---\n",
            "\n",
            "--- Training Start: CE | Noise: symmetric@80% | Epochs: 50 ---\n",
            "Using device: cuda\n",
            "Using Model: ResNet9\n",
            "Optimizer: SGD(lr=1.0e-01, momentum=0.9, wd=5.0e-04)\n",
            "Scheduler: CosineAnnealingLR(T_max=50)\n",
            "E 01/50 | L:3.6452 | TrA:  9.87% | TeA:  9.99% (B:  9.99%) | LR:0.100000\n",
            "E 02/50 | L:2.3044 | TrA:  9.85% | TeA:  9.98% (B:  9.99%) | LR:0.099901\n",
            "E 03/50 | L:2.3041 | TrA: 10.16% | TeA: 10.01% (B: 10.01%) | LR:0.099606\n",
            "E 04/50 | L:2.3042 | TrA:  9.86% | TeA: 10.00% (B: 10.01%) | LR:0.099114\n",
            "E 05/50 | L:2.3045 | TrA:  9.94% | TeA: 10.00% (B: 10.01%) | LR:0.098429\n",
            "E 06/50 | L:2.3047 | TrA:  9.69% | TeA: 10.01% (B: 10.01%) | LR:0.097553\n",
            "E 07/50 | L:2.3043 | TrA: 10.01% | TeA: 10.00% (B: 10.01%) | LR:0.096489\n",
            "E 08/50 | L:2.3042 | TrA:  9.99% | TeA: 10.00% (B: 10.01%) | LR:0.095241\n",
            "E 09/50 | L:2.3040 | TrA:  9.86% | TeA: 10.00% (B: 10.01%) | LR:0.093815\n",
            "E 10/50 | L:2.3039 | TrA: 10.05% | TeA: 10.01% (B: 10.01%) | LR:0.092216\n",
            "E 11/50 | L:2.3046 | TrA:  9.89% | TeA: 10.00% (B: 10.01%) | LR:0.090451\n",
            "E 12/50 | L:2.3043 | TrA:  9.97% | TeA: 10.01% (B: 10.01%) | LR:0.088526\n",
            "E 13/50 | L:2.3044 | TrA:  9.80% | TeA: 10.00% (B: 10.01%) | LR:0.086448\n",
            "E 14/50 | L:2.3042 | TrA:  9.93% | TeA: 10.00% (B: 10.01%) | LR:0.084227\n",
            "E 15/50 | L:2.3041 | TrA:  9.93% | TeA: 10.00% (B: 10.01%) | LR:0.081871\n",
            "E 16/50 | L:2.3039 | TrA: 10.05% | TeA: 10.00% (B: 10.01%) | LR:0.079389\n",
            "E 17/50 | L:2.3039 | TrA:  9.83% | TeA: 10.33% (B: 10.33%) | LR:0.076791\n",
            "E 18/50 | L:2.3031 | TrA: 10.31% | TeA: 15.53% (B: 15.53%) | LR:0.074088\n",
            "E 19/50 | L:2.3020 | TrA: 10.60% | TeA: 20.44% (B: 20.44%) | LR:0.071289\n",
            "E 20/50 | L:2.3008 | TrA: 11.03% | TeA: 16.46% (B: 20.44%) | LR:0.068406\n",
            "E 21/50 | L:2.3008 | TrA: 10.82% | TeA: 17.61% (B: 20.44%) | LR:0.065451\n",
            "E 22/50 | L:2.3006 | TrA: 10.92% | TeA: 15.18% (B: 20.44%) | LR:0.062434\n",
            "E 23/50 | L:2.3005 | TrA: 11.11% | TeA: 16.37% (B: 20.44%) | LR:0.059369\n",
            "E 24/50 | L:2.3000 | TrA: 11.11% | TeA: 18.29% (B: 20.44%) | LR:0.056267\n",
            "E 25/50 | L:2.2997 | TrA: 11.39% | TeA: 25.27% (B: 25.27%) | LR:0.053140\n",
            "E 26/50 | L:2.2994 | TrA: 11.48% | TeA: 22.91% (B: 25.27%) | LR:0.050000\n",
            "E 27/50 | L:2.2993 | TrA: 11.51% | TeA: 24.53% (B: 25.27%) | LR:0.046860\n",
            "E 28/50 | L:2.2989 | TrA: 11.57% | TeA: 20.85% (B: 25.27%) | LR:0.043733\n",
            "E 29/50 | L:2.2985 | TrA: 11.81% | TeA: 22.46% (B: 25.27%) | LR:0.040631\n",
            "E 30/50 | L:2.2978 | TrA: 11.86% | TeA: 24.76% (B: 25.27%) | LR:0.037566\n",
            "E 31/50 | L:2.2977 | TrA: 11.80% | TeA: 25.03% (B: 25.27%) | LR:0.034549\n",
            "E 32/50 | L:2.2978 | TrA: 11.88% | TeA: 27.07% (B: 27.07%) | LR:0.031594\n",
            "E 33/50 | L:2.2968 | TrA: 11.89% | TeA: 28.48% (B: 28.48%) | LR:0.028711\n",
            "E 34/50 | L:2.2964 | TrA: 12.11% | TeA: 27.78% (B: 28.48%) | LR:0.025912\n",
            "E 35/50 | L:2.2960 | TrA: 12.34% | TeA: 30.35% (B: 30.35%) | LR:0.023209\n",
            "E 36/50 | L:2.2955 | TrA: 12.32% | TeA: 32.81% (B: 32.81%) | LR:0.020611\n",
            "E 37/50 | L:2.2952 | TrA: 12.41% | TeA: 33.19% (B: 33.19%) | LR:0.018129\n",
            "E 38/50 | L:2.2944 | TrA: 12.55% | TeA: 33.91% (B: 33.91%) | LR:0.015773\n",
            "E 39/50 | L:2.2938 | TrA: 12.80% | TeA: 34.76% (B: 34.76%) | LR:0.013552\n",
            "E 40/50 | L:2.2931 | TrA: 13.21% | TeA: 34.07% (B: 34.76%) | LR:0.011474\n",
            "E 41/50 | L:2.2926 | TrA: 13.31% | TeA: 36.06% (B: 36.06%) | LR:0.009549\n",
            "E 42/50 | L:2.2917 | TrA: 13.45% | TeA: 38.22% (B: 38.22%) | LR:0.007784\n",
            "E 43/50 | L:2.2914 | TrA: 13.61% | TeA: 37.39% (B: 38.22%) | LR:0.006185\n",
            "E 44/50 | L:2.2906 | TrA: 13.48% | TeA: 38.42% (B: 38.42%) | LR:0.004759\n",
            "E 45/50 | L:2.2899 | TrA: 13.69% | TeA: 39.54% (B: 39.54%) | LR:0.003511\n",
            "E 46/50 | L:2.2895 | TrA: 13.95% | TeA: 39.35% (B: 39.54%) | LR:0.002447\n",
            "E 47/50 | L:2.2888 | TrA: 13.84% | TeA: 39.76% (B: 39.76%) | LR:0.001571\n",
            "E 48/50 | L:2.2881 | TrA: 14.03% | TeA: 40.44% (B: 40.44%) | LR:0.000886\n",
            "E 49/50 | L:2.2874 | TrA: 14.01% | TeA: 40.67% (B: 40.67%) | LR:0.000394\n",
            "E 50/50 | L:2.2875 | TrA: 14.26% | TeA: 40.36% (B: 40.67%) | LR:0.000099\n",
            "--- Training End: CE (Eta:0.80). Best Test Acc: 40.67%. Duration: 23.64 min ---\n",
            " --> Running NCE (s=1)(Sym)...\n",
            "\n",
            "--- Training Start: NCE (s=1) | Noise: symmetric@0% | Epochs: 50 ---\n",
            "Using device: cuda\n",
            "Using Model: ResNet9\n",
            "Optimizer: SGD(lr=1.0e-01, momentum=0.9, wd=5.0e-04)\n",
            "Scheduler: CosineAnnealingLR(T_max=50)\n",
            "E 01/50 | L:0.0247 | TrA: 43.04% | TeA: 51.19% (B: 51.19%) | LR:0.100000\n",
            "E 02/50 | L:0.0125 | TrA: 58.56% | TeA: 56.00% (B: 56.00%) | LR:0.099901\n",
            "E 03/50 | L:0.0096 | TrA: 65.50% | TeA: 62.15% (B: 62.15%) | LR:0.099606\n",
            "E 04/50 | L:0.0084 | TrA: 69.80% | TeA: 69.33% (B: 69.33%) | LR:0.099114\n",
            "E 05/50 | L:0.0076 | TrA: 72.78% | TeA: 74.43% (B: 74.43%) | LR:0.098429\n",
            "E 06/50 | L:0.0073 | TrA: 74.80% | TeA: 72.19% (B: 74.43%) | LR:0.097553\n",
            "E 07/50 | L:0.0071 | TrA: 76.92% | TeA: 72.22% (B: 74.43%) | LR:0.096489\n",
            "E 08/50 | L:0.0070 | TrA: 77.95% | TeA: 71.70% (B: 74.43%) | LR:0.095241\n",
            "E 09/50 | L:0.0068 | TrA: 79.35% | TeA: 69.96% (B: 74.43%) | LR:0.093815\n",
            "E 10/50 | L:0.0067 | TrA: 80.69% | TeA: 76.38% (B: 76.38%) | LR:0.092216\n",
            "E 11/50 | L:0.0066 | TrA: 81.88% | TeA: 78.37% (B: 78.37%) | LR:0.090451\n",
            "E 12/50 | L:0.0064 | TrA: 82.86% | TeA: 78.89% (B: 78.89%) | LR:0.088526\n",
            "E 13/50 | L:0.0062 | TrA: 83.34% | TeA: 76.08% (B: 78.89%) | LR:0.086448\n",
            "E 14/50 | L:0.0062 | TrA: 83.98% | TeA: 81.12% (B: 81.12%) | LR:0.084227\n",
            "E 15/50 | L:0.0060 | TrA: 84.64% | TeA: 78.43% (B: 81.12%) | LR:0.081871\n",
            "E 16/50 | L:0.0059 | TrA: 85.19% | TeA: 79.38% (B: 81.12%) | LR:0.079389\n",
            "E 17/50 | L:0.0059 | TrA: 85.50% | TeA: 80.40% (B: 81.12%) | LR:0.076791\n",
            "E 18/50 | L:0.0057 | TrA: 86.36% | TeA: 78.72% (B: 81.12%) | LR:0.074088\n",
            "E 19/50 | L:0.0054 | TrA: 87.03% | TeA: 85.30% (B: 85.30%) | LR:0.071289\n",
            "E 20/50 | L:0.0052 | TrA: 87.20% | TeA: 83.61% (B: 85.30%) | LR:0.068406\n",
            "E 21/50 | L:0.0050 | TrA: 87.95% | TeA: 81.71% (B: 85.30%) | LR:0.065451\n",
            "E 22/50 | L:0.0048 | TrA: 88.51% | TeA: 81.16% (B: 85.30%) | LR:0.062434\n",
            "E 23/50 | L:0.0046 | TrA: 88.81% | TeA: 86.87% (B: 86.87%) | LR:0.059369\n",
            "E 24/50 | L:0.0043 | TrA: 89.73% | TeA: 85.02% (B: 86.87%) | LR:0.056267\n",
            "E 25/50 | L:0.0042 | TrA: 90.17% | TeA: 82.42% (B: 86.87%) | LR:0.053140\n",
            "E 26/50 | L:0.0040 | TrA: 90.46% | TeA: 85.33% (B: 86.87%) | LR:0.050000\n",
            "E 27/50 | L:0.0040 | TrA: 90.79% | TeA: 87.30% (B: 87.30%) | LR:0.046860\n",
            "E 28/50 | L:0.0036 | TrA: 91.37% | TeA: 84.78% (B: 87.30%) | LR:0.043733\n",
            "E 29/50 | L:0.0034 | TrA: 92.12% | TeA: 88.16% (B: 88.16%) | LR:0.040631\n",
            "E 30/50 | L:0.0031 | TrA: 92.84% | TeA: 86.78% (B: 88.16%) | LR:0.037566\n",
            "E 31/50 | L:0.0030 | TrA: 93.19% | TeA: 87.22% (B: 88.16%) | LR:0.034549\n",
            "E 32/50 | L:0.0028 | TrA: 93.67% | TeA: 89.81% (B: 89.81%) | LR:0.031594\n",
            "E 33/50 | L:0.0026 | TrA: 94.38% | TeA: 89.11% (B: 89.81%) | LR:0.028711\n",
            "E 34/50 | L:0.0023 | TrA: 95.02% | TeA: 88.94% (B: 89.81%) | LR:0.025912\n",
            "E 35/50 | L:0.0020 | TrA: 95.80% | TeA: 90.12% (B: 90.12%) | LR:0.023209\n",
            "E 36/50 | L:0.0018 | TrA: 96.24% | TeA: 90.67% (B: 90.67%) | LR:0.020611\n",
            "E 37/50 | L:0.0016 | TrA: 96.88% | TeA: 90.75% (B: 90.75%) | LR:0.018129\n"
          ]
        }
      ]
    }
  ]
}